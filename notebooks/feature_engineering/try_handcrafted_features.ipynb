{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:00.389730Z",
     "start_time": "2024-03-19T13:01:59.092830Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.modules.training.datasets.main_dataset import MainDataset\n",
    "from src.utils.setup import setup_data\n",
    "# Import all important ML packages.\n",
    "from src.utils.stratified_splitter import create_stratified_cv_splits\n",
    "import pickle\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EEG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3788bad076c577a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load in all the data and create a dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "252b78e6100b920"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Sample 10000 random indices from the training dataset and 2000 from the validation dataset.\n",
    "random.seed(42)\n",
    "\n",
    "#Pickle X_train, X_test, y_train, y_test  as 1 tuple if it does not exist yet.\n",
    "n_samples = 5000\n",
    "\n",
    "path = \"../../data/raw\"\n",
    "\n",
    "if not os.path.exists(\"train_test20k.pkl\") or n_samples != 20000:\n",
    "    #Start by setting up the data. Do not run if you already have the pickled data. \n",
    "\n",
    "    X, y = setup_data(path)\n",
    "    \n",
    "    #Create a dataset\n",
    "    train_indices, test_indices = create_stratified_cv_splits(X.meta, y, int(1 / 0.2))[0]\n",
    "    \n",
    "    dataset = MainDataset(\"eeg\", X, y, indices=list(range(len(y))))\n",
    "    train_sample = random.sample(list(train_indices), n_samples)\n",
    "    val_sample = random.sample(list(test_indices), n_samples)\n",
    "\n",
    "    X_train = np.array([dataset.__getitem__(i)[0] for i in train_sample])\n",
    "    y_train = np.array([dataset.__getitem__(i)[1] for i in train_sample])\n",
    "\n",
    "    X_test = np.array([dataset.__getitem__(i)[0] for i in val_sample])\n",
    "    y_test = np.array([dataset.__getitem__(i)[1] for i in val_sample])\n",
    "    with open(\"train_test20k.pkl\", \"wb\") as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(\"train_test20k.pkl\", \"rb\") as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.630991Z",
     "start_time": "2024-03-19T13:02:00.390682Z"
    }
   },
   "id": "a3c76418d9f5bc5",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90898ba1587cfdce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Let's try statistical features such as rolling mean to start with on all channels \n",
    "# and then try to use the rolling mean on the channels that are most important."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.633614Z",
     "start_time": "2024-03-19T13:02:23.631917Z"
    }
   },
   "id": "e98bc341fc99903a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def channel_selection(data, channels):\n",
    "    \"\"\"\n",
    "    Selects the specified channels from a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).ff\n",
    "        channels (list): The list of channels to select.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The data with the selected channels.\n",
    "    \"\"\"\n",
    "    return data[:, :, channels]\n",
    "\n",
    "\n",
    "def apply_downsampling(data, factor):\n",
    "    \"\"\"\n",
    "    Downsamples a 3D numpy array by averaging over windows of the specified factor.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        factor (int): The downsampling factor.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The downsampled data.\n",
    "    \"\"\"\n",
    "    reshaped_data = data.reshape(data.shape[0], data.shape[1] // factor, factor, data.shape[2])\n",
    "\n",
    "    # Compute the mean along the new axis that represents the blocks to be averaged\n",
    "    downsampled_data = reshaped_data.mean(axis=2)\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "\n",
    "def apply_rolling_operation(data, window_size, operation):\n",
    "    \"\"\"\n",
    "    Applies a rolling operation (e.g., mean, std, min, max) to a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        window_size (int): The size of the rolling window.\n",
    "        operation (callable): The operation to apply within the rolling window.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The data with the rolling operation applied.\n",
    "    \"\"\"\n",
    "    # Initialize the result array with NaNs to preserve original shape\n",
    "    result = np.full(data.shape, np.nan)\n",
    "\n",
    "    # Apply the rolling operation to each sample and channel\n",
    "    for sample_idx in tqdm(range(data.shape[0])):\n",
    "        for channel_idx in range(data.shape[2]):\n",
    "            sequence = data[sample_idx, :, channel_idx]\n",
    "            # Use sliding_window_view to create rolling windows\n",
    "            windows = sliding_window_view(sequence, window_shape=window_size)\n",
    "            # Apply the operation across the windows' axis (axis=-1) and assign to the result\n",
    "            # Center the result of the operation in the middle of the window\n",
    "            operation_result = operation(windows, axis=-1)\n",
    "            start_index = window_size // 2\n",
    "            end_index = start_index + operation_result.shape[0]\n",
    "            result[sample_idx, start_index:end_index, channel_idx] = operation_result\n",
    "\n",
    "            #Forward and backward fill the NaNs\n",
    "            result[sample_idx, :, channel_idx] = pd.Series(result[sample_idx, :, channel_idx]).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def rolling_plot(X_train, X_test, window_size, operation):\n",
    "    #Create a plot before and after.\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "    #Plot the original data for the first sample and channel\n",
    "    ax[0].plot(X_train[0, :, 0])\n",
    "    ax[0].set_title(\"Original data\")\n",
    "\n",
    "    # Assuming X_train and X_test are your datasets\n",
    "    X_train = apply_rolling_operation(X_train, window_size, operation)\n",
    "    X_test = apply_rolling_operation(X_test, window_size, operation)\n",
    "\n",
    "    #Plot the transformed data for the first sample and channel\n",
    "    ax[1].plot(X_train[0, :, 0])\n",
    "    ax[1].set_title(f\"Transformed data - rolling {str(operation)} with window size {window_size}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return X_train, X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.730289Z",
     "start_time": "2024-03-19T13:02:23.634528Z"
    }
   },
   "id": "79a4843c923ad206",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c95071060c091b30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Remove the time dimension by flattening the data from (n, sequence_length, channels) to (n*sequence_length, channels).\n",
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    sequence_length = X_train.shape[1]\n",
    "\n",
    "    X_train = X_train.reshape(-1, X_train.shape[2])\n",
    "    X_test = X_test.reshape(-1, X_test.shape[2])\n",
    "\n",
    "    #Also process the labels from size (n, channels) to (n*sequence_length, channels).\n",
    "    y_train = np.repeat(y_train, repeats=sequence_length, axis=0)\n",
    "    y_test = np.repeat(y_test, repeats=sequence_length, axis=0)\n",
    "\n",
    "    #Clip the data between -1024 and 1024.\n",
    "    X_train = np.clip(X_train, -1024, 1024)\n",
    "    X_test = np.clip(X_test, -1024, 1024)\n",
    "\n",
    "    #Divide the data by 32\n",
    "    X_train = X_train / 32\n",
    "    X_test = X_test / 32\n",
    "\n",
    "    #Set NaN to 0\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "\n",
    "    #Divide the labels by the sum of the labels to get a probability distribution.\n",
    "    y_train = y_train / np.sum(y_train, axis=1).reshape(-1, 1)\n",
    "    y_test = y_test / np.sum(y_test, axis=1).reshape(-1, 1)\n",
    "\n",
    "    #Randomly shuffle train data\n",
    "    np.random.seed(42)\n",
    "    p = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[p]\n",
    "    y_train = y_train[p]\n",
    "\n",
    "    #One hot encode the labels\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.772197Z",
     "start_time": "2024-03-19T13:02:23.731085Z"
    }
   },
   "id": "c65eff39e14c08a9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test):\n",
    "    #Create a model\n",
    "    #model = xgb.XGBClassifier(num_class=6, max_depth=6, learning_rate=0.1, n_estimators=50,  verbosity=1, early_stopping_rounds=10)\n",
    "    #Train a lightgbm model\n",
    "    model = lgb.LGBMClassifier(num_class=6, max_depth=10, n_estimators=100,  verbosity=1, early_stopping_rounds=10)\n",
    "    # Use early stopping to stop the training if the validation score does not improve for 10 rounds.\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:53:43.928853Z",
     "start_time": "2024-03-19T13:53:43.926463Z"
    }
   },
   "id": "d0b51909633014b9",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Calculate the accuracy and create a confusion matrix with seaborn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.scoring.kldiv import KLDiv\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred, disable_plot=False):\n",
    "\n",
    "\n",
    "    #Set the axis labels\n",
    "    #Create the confusion matrix\n",
    "    #Take the argmax of the probabilities to get the predicted class\n",
    "    y_test_final = np.argmax(y_test, axis=1)\n",
    "    y_pred_final = np.argmax(y_pred, axis=1)\n",
    "    kldiv = KLDiv()\n",
    "    score = kldiv(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test_final, y_pred_final)\n",
    "\n",
    "    \n",
    "    \n",
    "    if not disable_plot:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        print(f\"KLDiv: {score}\")\n",
    "        print(f\"Classification accuracy:  {accuracy}\")\n",
    "        sns.heatmap(confusion_matrix(y_test_final, y_pred_final), annot=True, fmt=\"d\", ax=ax)\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "    \n",
    "        #Set the labels\n",
    "        ax.set_xticklabels([\"Seizure\", \"Lpd\", \"Gpd\", \"Lrda\", \"Grda\", \"Other\"])\n",
    "        ax.set_yticklabels([\"Seizure\", \"Lpd\", \"Gpd\", \"Lrda\", \"Grda\", \"Other\"])\n",
    "        plt.show()\n",
    "    return score, accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.857993Z",
     "start_time": "2024-03-19T13:02:23.829822Z"
    }
   },
   "id": "ca1a51cfa73a4ee1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names, tsfresh=False):\n",
    "    #Plot the feature importances with xgboost horizontally\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(35, 10))\n",
    "\n",
    "    #Check if model is of type xgboost\n",
    "    if isinstance(model, xgb.XGBClassifier):\n",
    "        xgb.plot_importance(model, ax=ax[0], importance_type=\"gain\")\n",
    "    elif isinstance(model, lgb.LGBMClassifier):\n",
    "        lgb.plot_importance(model, ax=ax[0], importance_type=\"gain\")\n",
    "        \n",
    "    if not tsfresh:\n",
    "        ax[0].set_yticklabels(feature_names)\n",
    "\n",
    "    #Show plots/brain.png on the right\n",
    "    ax[1].imshow(plt.imread(\"../../plots/brain.png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b28ed8f109f98fc3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pipeline(X_train, X_test, y_train, y_test):\n",
    "    #Channel selection\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    selected_c = all_channels\n",
    "    channels = [channel_dict[c] for c in selected_c]\n",
    "    window_size = 175\n",
    "    operation = np.std\n",
    "\n",
    "    #Select the channels\n",
    "    X_train = channel_selection(X_train, channels)\n",
    "    X_test = channel_selection(X_test, channels)\n",
    "\n",
    "    #Downsample the data\n",
    "    X_train = apply_downsampling(X_train, 10)\n",
    "    X_test = apply_downsampling(X_test, 10)\n",
    "\n",
    "    #Apply the rolling operation\n",
    "    X_train, X_test = rolling_plot(X_train, X_test, window_size, operation)\n",
    "\n",
    "    #Now preprocess the data\n",
    "    X_train, X_test, y_train, y_test = preprocess(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #Train the model\n",
    "    model = train(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Plot the confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #Plot the feature importances\n",
    "    feature_names = np.array(X.eeg[list(X.eeg.keys())[0]].columns)[channels]\n",
    "    plot_feature_importances(model, feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.943730Z",
     "start_time": "2024-03-19T13:02:23.900969Z"
    }
   },
   "id": "a979af24a1eeba38",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Run the pipeline\n",
    "#pipeline(X_train, X_test, y_train, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:23.989078Z",
     "start_time": "2024-03-19T13:02:23.944813Z"
    }
   },
   "id": "725af528db6c8d7a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Now lets use tsfresh for feature engineering\n",
    "\n",
    "def preprocess_to_df(X, all_channels):\n",
    "    \n",
    "    #Clip the data between -1024 and 1024.\n",
    "    X = np.clip(X, -1024, 1024)\n",
    "    #Divide the data by 32\n",
    "    X = X / 32\n",
    "    #Set NaN to 0\n",
    "    X = np.nan_to_num(X)\n",
    "    id = np.repeat(range(X.shape[0]), X.shape[1])\n",
    "    X = X.reshape(-1, X.shape[2])\n",
    "    df = pd.DataFrame(X, columns=all_channels)\n",
    "    del X\n",
    "    df[\"id\"] = id\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def apply_rolling(X: pd.DataFrame, window_sizes: list, operations: list):\n",
    "    \"\"\"\n",
    "    Applies a rolling operation (e.g., mean, std, min, max) to a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        window_size (int): The size of the rolling window.\n",
    "        operation (callable): The operation to apply within the rolling window.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The data with the rolling operation applied.\n",
    "    \"\"\"\n",
    "    # Initialize the result array with NaNs to preserve original shape\n",
    "    result = pd.DataFrame()\n",
    "    # Apply the rolling operation to each sample and channel (exclude Id)\n",
    "    for channel, window_size, operation in zip(X.columns[:-1], window_sizes, operations): \n",
    "        #Convert operation from string to callable\n",
    "        result[channel] = X[channel].groupby(X[\"id\"]).rolling(window=window_size).agg(operation).ffill().bfill().astype(np.float32)\n",
    "        #Rename the current column\n",
    "        result.rename(columns={channel: f\"{channel}_{operation}_{window_size}\"}, inplace=True)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_tf(y):\n",
    "    #One hot encode the labels\n",
    "    y = np.argmax(y, axis=1)\n",
    "    #Repeat y based on \n",
    "    return y\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:24.035306Z",
     "start_time": "2024-03-19T13:02:23.989634Z"
    }
   },
   "id": "20a3cd91923b4a19",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tsfresh import select_features\n",
    "\n",
    "\n",
    "def pipeline_single(X_train, X_test, y_train, y_test):\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    unary_ops = ['abs']\n",
    "    stats = ['mean', 'std', 'min', 'max', 'median', 'skew','kurt']\n",
    "    window_sizes = [6, 12, 36, 50, 90, 180, 360, 500, 1000]\n",
    "    downsample = 10\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    \n",
    "    scores = dict()\n",
    "    accuracies = dict()\n",
    "    def objective(trial):\n",
    "       \n",
    "        #Select a random number of  channels, for now lets do one.\n",
    "        # num_channels = random.randint(1, 20)\n",
    "        num_channels = 1\n",
    "        \n",
    "        #Select the channels using optuna\n",
    "        selected_c = trial.suggest_categorical(\"channels\", all_channels)\n",
    "        channels = [channel_dict[selected_c]]\n",
    "        \n",
    "        # Select window size\n",
    "        window_size = trial.suggest_categorical(\"window_size\", window_sizes)\n",
    "        \n",
    "        # Select operation\n",
    "        operation = trial.suggest_categorical(\"operation\", stats)\n",
    "        \n",
    "        #Select the channels\n",
    "        X_train_curr = channel_selection(X_train, channels)\n",
    "        X_test_curr = channel_selection(X_test, channels)\n",
    "        \n",
    "        #Trial boolean for abs\n",
    "        unary = trial.suggest_categorical(\"abs\", [True, False])\n",
    "        if unary:\n",
    "            X_train_curr = np.abs(X_train_curr)\n",
    "            X_test_curr = np.abs(X_test_curr)\n",
    "        \n",
    "        #Repeat y based on the sequence length\n",
    "        y_train_curr = np.repeat(y_train, repeats=X_train.shape[1] / downsample, axis=0)\n",
    "        y_test_curr = np.repeat(y_test, repeats=X_test.shape[1] / downsample, axis=0)\n",
    "    \n",
    "        #Downsample the data\n",
    "        X_train_curr = apply_downsampling(X_train_curr, downsample)\n",
    "        X_test_curr = apply_downsampling(X_test_curr, downsample)\n",
    "    \n",
    "\n",
    "        #Apply the rolling operation\n",
    "        X_train_curr = preprocess_to_df(X_train_curr, [selected_c])\n",
    "        X_test_curr = preprocess_to_df(X_test_curr, [selected_c])\n",
    "        \n",
    "        X_train_curr = apply_rolling(X_train_curr, [window_size], [operation])\n",
    "        X_test_curr = apply_rolling(X_test_curr, [window_size], [operation])\n",
    "        \n",
    "        #Now preprocess the data\n",
    "        y_train_curr_final = preprocess_tf(y_train_curr)\n",
    "        y_test_curr_final = preprocess_tf(y_test_curr)\n",
    "        \n",
    "        #Train the model\n",
    "        model = train(X_train_curr, X_test_curr, y_train_curr_final, y_test_curr_final)\n",
    "    \n",
    "        #Make predictions with probabilities\n",
    "        y_pred = model.predict_proba(X_test_curr)\n",
    "    \n",
    "        #Plot the confusion matrix\n",
    "        score, accuracy = plot_confusion_matrix(y_test_curr, y_pred, disable_plot=True)\n",
    "        \n",
    "        #Save the scores\n",
    "        scores[f\"{selected_c[0]}-{downsample}-{window_size}-{operation}\"] = score\n",
    "        accuracies[f\"{downsample}-{window_size}-{operation}\"] = accuracy\n",
    "        \n",
    "        return score\n",
    "    return objective\n",
    "\n",
    "#Train a config with these keys\n",
    "def train_config(X_train, X_test, y_train, y_test, config):\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    \n",
    "    curr_channels = [channel_dict[c.split(\"-\")[1]] for c in config[0]]\n",
    "    \n",
    "    #Select the channels\n",
    "    X_train_curr = channel_selection(X_train, curr_channels)\n",
    "    X_test_curr = channel_selection(X_test, curr_channels)\n",
    "    \n",
    "    #Repeat y based on the sequence length\n",
    "    y_train_curr = np.repeat(y_train, repeats=X_train.shape[1] / config[1][0], axis=0)\n",
    "    y_test_curr = np.repeat(y_test, repeats=X_test.shape[1] / config[1][0], axis=0)\n",
    "    \n",
    "    #Downsample the data\n",
    "    X_train_curr = apply_downsampling(X_train_curr, config[1][0])\n",
    "    X_test_curr = apply_downsampling(X_test_curr, config[1][0])\n",
    "    \n",
    "    #Apply the rolling operation\n",
    "    X_train_curr = preprocess_to_df(X_train_curr, config[0])\n",
    "    X_test_curr = preprocess_to_df(X_test_curr, config[0])\n",
    "    X_train_curr = apply_rolling(X_train_curr, config[2], config[3])\n",
    "    X_test_curr = apply_rolling(X_test_curr, config[2], config[3])\n",
    "    \n",
    "    #Now preprocess the data\n",
    "    y_train_curr_final = preprocess_tf(y_train_curr)\n",
    "    y_test_curr_final = preprocess_tf(y_test_curr)\n",
    "    \n",
    "    #Train the model\n",
    "    model = train(X_train_curr, X_test_curr, y_train_curr_final, y_test_curr_final)\n",
    "    \n",
    "    #Make predictions with probabilities\n",
    "    y_pred = model.predict_proba(X_test_curr)\n",
    "    \n",
    "    #Plot the confusion matrix\n",
    "    score, accuracy = plot_confusion_matrix(y_test_curr, y_pred, disable_plot=False)\n",
    "    \n",
    "    #Plot the feature importances\n",
    "    feature_names = [c[1:] for c in X_train_curr.columns]\n",
    "    plot_feature_importances(model, feature_names, tsfresh=True)\n",
    "    return score, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.261966Z",
     "start_time": "2024-03-19T13:02:24.035914Z"
    }
   },
   "id": "e4e8928597678316",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OPTUNA - Double channel search\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bb268a3682eb3e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pipeline_double(X_train, X_test, y_train, y_test):\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    operations = [\"/\", \"+\", \"-\", \"*\"]\n",
    "    stats = ['mean', 'std', 'min', 'max', 'median', 'skew','kurt']\n",
    "    window_sizes = [5, 12, 25,40, 50, 100, 200]\n",
    "    downsample = 10\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    \n",
    "    #Create plots folder if it does not exist\n",
    "    if not os.path.exists(\"plots\"):\n",
    "        os.makedirs(\"plots\")\n",
    "    \n",
    "    cache = {}\n",
    "    \n",
    "    def objective(trial):\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "        \n",
    "        #Suggest all for the trial\n",
    "        c1 = trial.suggest_categorical(\"c1\", all_channels)\n",
    "        c2 = trial.suggest_categorical(\"c2\", all_channels)\n",
    "        op = trial.suggest_categorical(\"operation\", operations)\n",
    "        # Select window size\n",
    "        window_size = trial.suggest_categorical(\"window_size\", window_sizes)\n",
    "        # Select operation\n",
    "        operation = trial.suggest_categorical(\"rolling\", stats)\n",
    "        \n",
    "        all_params = str([c1, c2, op, window_size, operation])\n",
    "        if all_params in cache:\n",
    "            return cache[all_params]\n",
    "    \n",
    "        curr_c1 = [channel_dict[c1]]\n",
    "        X_train_c1 = channel_selection(X_train, curr_c1)\n",
    "        \n",
    "        ax[0].plot(X_train_c1[0, :, 0], label=f\"{c1}\")\n",
    "        \n",
    "        #Do the same for the second channel\n",
    "\n",
    "        curr_c2 = [channel_dict[c2]]\n",
    "        X_train_c2 = channel_selection(X_train, curr_c2)\n",
    "        \n",
    "        ax[0].plot(X_train_c2[0, :, 0], label=f\"{c2}\")\n",
    "        #Do the same for test\n",
    "        X_test_c1 = channel_selection(X_test, curr_c1)\n",
    "        X_test_c2 = channel_selection(X_test, curr_c2)\n",
    "        \n",
    "        #Apply the operation\n",
    "        X_train_curr = apply_op(X_train_c1, X_train_c2, op)\n",
    "        X_test_curr = apply_op(X_test_c1, X_test_c2, op)\n",
    " \n",
    "        ax[0].plot(X_train_curr[0, :, 0], label=f\"{c1}{op}{c2}\")\n",
    "        ax[0].legend()\n",
    "\n",
    "        #Repeat y based on the sequence length\n",
    "        repeats = X_train.shape[1] // downsample\n",
    "        y_train_curr = np.repeat(y_train, repeats=repeats, axis=0)\n",
    "        y_test_curr = np.repeat(y_test, repeats=repeats, axis=0)\n",
    "    \n",
    "        #Downsample the data\n",
    "        X_train_curr = apply_downsampling(X_train_curr, downsample)\n",
    "        X_test_curr = apply_downsampling(X_test_curr, downsample)\n",
    "    \n",
    "        #Apply the rolling operation\n",
    "        X_train_curr = preprocess_to_df(X_train_curr, [f\"{c1}{op}{c2}\"])\n",
    "        X_test_curr = preprocess_to_df(X_test_curr, [f\"{c1}{op}{c2}\"])\n",
    "        \n",
    "        X_train_curr = apply_rolling(X_train_curr, [window_size, window_size], [operation, operation])\n",
    "        X_test_curr = apply_rolling(X_test_curr, [window_size, window_size], [operation, operation])\n",
    "        \n",
    "        #Now preprocess the data\n",
    "        y_train_curr_final = preprocess_tf(y_train_curr)\n",
    "        y_test_curr_final = preprocess_tf(y_test_curr)\n",
    "        \n",
    "        #Train the model\n",
    "        model = train(X_train_curr, X_test_curr, y_train_curr_final, y_test_curr_final)\n",
    "    \n",
    "        #Make predictions with probabilities\n",
    "        y_pred = model.predict_proba(X_test_curr)\n",
    "    \n",
    "        #Plot the confusion matrix\n",
    "        score, accuracy = plot_confusion_matrix(y_test_curr, y_pred, disable_plot=True)\n",
    "        \n",
    "        #Add to cache\n",
    "        cache[all_params] = score\n",
    "        \n",
    "        #Plot the transformed data for the first sample and channel\n",
    "        ax[1].plot(X_train_curr.iloc[:repeats].to_numpy().flatten(), label=f\"{c1}{op}{c2} with window size {window_size} and operation {operation}\")\n",
    "        ax[1].legend()\n",
    "        \n",
    "        ax[0].set_title(f\"Plot with score {score} and accuracy {accuracy}.\")\n",
    "        #Save plot to file based on the trial count + score#\n",
    "        plt.savefig(f\"plots/{trial.number}_{score}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        return score\n",
    "    return objective\n",
    "\n",
    "def apply_op(X_c1, X_c2, operation):\n",
    "    print(X_c1.shape)\n",
    "    print(X_c2.shape)\n",
    "    #Apply the operation \n",
    "    if operation == \"/\":\n",
    "        return X_c1 / X_c2\n",
    "    elif operation == \"+\":\n",
    "        return X_c1 + X_c2\n",
    "    elif operation == \"-\":\n",
    "        return X_c1 - X_c2\n",
    "    elif operation == \"*\":\n",
    "        return X_c1 * X_c2\n",
    "    \n",
    "def apply_op_channels(X_c1, X_c2, operation):\n",
    "    #Apply the operation for each channel\n",
    "    #Create an empty array\n",
    "    result = np.zeros(X_c1.shape)\n",
    "    for i, op in enumerate(operation):\n",
    "        if op == \"/\":\n",
    "            result[:, :, i] = X_c1[:, :, i] / X_c2[:, :, i]\n",
    "        elif op == \"+\":\n",
    "            result[:, :, i] = X_c1[:, :, i] + X_c2[:, :, i]\n",
    "        elif op == \"-\":\n",
    "            result[:, :, i] = X_c1[:, :, i] - X_c2[:, :, i]\n",
    "        elif op == \"*\":\n",
    "            result[:, :, i] = X_c1[:, :, i] * X_c2[:, :, i]\n",
    "    return result\n",
    "        \n",
    "       \n",
    "#Train a config with these keys\n",
    "def train_config_double(X_train, X_test, y_train, y_test, config):\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    \n",
    "    curr_c1s = [channel_dict[c.split(\"-\")[1]] for c in config[0]]\n",
    "    curr_c2s = [channel_dict[c.split(\"-\")[1]] for c in config[1]]\n",
    "    \n",
    "    #Select the channels\n",
    "    X_train1_curr = channel_selection(X_train, curr_c1s)\n",
    "    X_train2_curr = channel_selection(X_train, curr_c2s)\n",
    "    \n",
    "    X_test1_curr = channel_selection(X_test, curr_c1s)\n",
    "    X_test2_curr = channel_selection(X_test, curr_c2s)\n",
    "    \n",
    "    #Apply the operation\n",
    "    X_train_curr = apply_op_channels(X_train1_curr, X_train2_curr, config[3])\n",
    "    X_test_curr = apply_op_channels(X_test1_curr, X_test2_curr, config[3])\n",
    "    \n",
    "    del X_train1_curr, X_train2_curr, X_test1_curr, X_test2_curr\n",
    "    #Repeat y based on the sequence length\n",
    "    y_train_curr = np.repeat(y_train, repeats=X_train.shape[1] / config[2][0], axis=0)\n",
    "    y_test_curr = np.repeat(y_test, repeats=X_test.shape[1] / config[2][0], axis=0)\n",
    "    \n",
    "    #Downsample the data\n",
    "    X_train_curr = apply_downsampling(X_train_curr, config[2][0])\n",
    "    X_test_curr = apply_downsampling(X_test_curr, config[2][0])\n",
    "    \n",
    "    #Apply the rolling operation\n",
    "    X_train_curr = preprocess_to_df(X_train_curr, [f\"{c1}{op}{c2}\" for c1, c2, op in zip(config[0], config[1], config[3])])\n",
    "    X_test_curr = preprocess_to_df(X_test_curr, [f\"{c1}{op}{c2}\" for c1, c2, op in zip(config[0], config[1], config[3])])\n",
    "    \n",
    "    X_train_curr = apply_rolling(X_train_curr, config[4], config[5])\n",
    "    X_test_curr = apply_rolling(X_test_curr, config[4], config[5])\n",
    "    \n",
    "    #Now preprocess the data\n",
    "    y_train_curr_final = preprocess_tf(y_train_curr)\n",
    "    y_test_curr_final = preprocess_tf(y_test_curr)\n",
    "    \n",
    "    #Train the model\n",
    "    model = train(X_train_curr, X_test_curr, y_train_curr_final, y_test_curr_final)\n",
    "    \n",
    "    #Make predictions with probabilities\n",
    "    y_pred = model.predict_proba(X_test_curr)\n",
    "    \n",
    "    #Plot the confusion matrix\n",
    "    score, accuracy = plot_confusion_matrix(y_test_curr, y_pred, disable_plot=False)\n",
    "    \n",
    "    #Plot the feature importances\n",
    "    feature_names = [c[1:] for c in X_train_curr.columns]\n",
    "    plot_feature_importances(model, feature_names, tsfresh=True)\n",
    "    return score, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:10:44.339266Z",
     "start_time": "2024-03-19T13:10:44.329833Z"
    }
   },
   "id": "90bdfff6917c1d",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OPTUNA - Single channel search\n",
    "\n",
    "Uncomment if you want to run."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a4ad480f3c0326"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 14:02:25,471] Using an existing study with name 'single_channel' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import optuna\n",
    "\n",
    "optuna_storage = \"sqlite:///optuna.db\"\n",
    "study = optuna.create_study(study_name=\"single_channel\", storage=optuna_storage, load_if_exists=True, direction=\"minimize\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.475789Z",
     "start_time": "2024-03-19T13:02:25.272008Z"
    }
   },
   "id": "a3ef02d371a47f28",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#study.optimize(pipeline_single(X_train, X_test, y_train, y_test), n_trials=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.478092Z",
     "start_time": "2024-03-19T13:02:25.476533Z"
    }
   },
   "id": "253b1b91d5a3fd1",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Get the best runs from the study\n",
    "# sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "# \n",
    "# # Filter out duplicates based on parameter values\n",
    "# seen_params = set()\n",
    "# unique_trials = []\n",
    "# for trial in sorted_trials:\n",
    "#     # Create a unique representation of the trial's parameters (e.g., as a tuple of sorted items)\n",
    "#     params_repr = tuple(sorted(trial.params.items()))\n",
    "#     if params_repr not in seen_params:\n",
    "#         unique_trials.append(trial)\n",
    "#         seen_params.add(params_repr)\n",
    "#     \n",
    "#     # Stop once we have collected 10 unique trials\n",
    "#     if len(unique_trials) == 50:\n",
    "#         break\n",
    "# \n",
    "# top_10 = [trial.params for trial in unique_trials]\n",
    "# \n",
    "# config_channels = [str(i) + \"-\" + config[\"channels\"]for i,config in zip(range(len(top_10)), top_10)]\n",
    "# config_downsample = [10] * 15\n",
    "# config_window = [config[\"window_size\"] for config in top_10]\n",
    "# config_operation = [config[\"operation\"] for config in top_10]\n",
    "# config = [config_channels, config_downsample, config_window, config_operation]\n",
    "# train_config(X_train, X_test, y_train, y_test, config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.502902Z",
     "start_time": "2024-03-19T13:02:25.478651Z"
    }
   },
   "id": "f69684c39a48fc7d",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OPTUNA Double channel search run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e6e580a18c4e5c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 14:02:25,539] Using an existing study with name 'double_channel' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "optuna_storage = \"sqlite:///optuna.db\"\n",
    "study = optuna.create_study(study_name=\"double_channel\", storage=optuna_storage, load_if_exists=True, direction=\"minimize\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.542982Z",
     "start_time": "2024-03-19T13:02:25.503509Z"
    }
   },
   "id": "32760b75695fefb8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#study.optimize(pipeline_double(X_train, X_test, y_train, y_test), n_trials=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:02:25.552721Z",
     "start_time": "2024-03-19T13:02:25.543571Z"
    }
   },
   "id": "f5c1294ee83f7890",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10000, 60)\n",
      "(5000, 10000, 60)\n"
     ]
    }
   ],
   "source": [
    "#Set the number of runs\n",
    "n = 40\n",
    "\n",
    "#Get the best runs from the study\n",
    "sorted_trials = sorted(study.trials, key=lambda t: t.value if t.value is not None else float('inf'))\n",
    "\n",
    "# Filter out duplicates based on parameter values\n",
    "seen_params = set()\n",
    "unique_trials = []\n",
    "for trial in sorted_trials:\n",
    "    # Create a unique representation of the trial's parameters (e.g., as a tuple of sorted items)\n",
    "    params_repr = tuple(sorted(trial.params.items()))\n",
    "    if params_repr not in seen_params:\n",
    "        unique_trials.append(trial)\n",
    "        seen_params.add(params_repr)\n",
    "    \n",
    "    # Stop once we have collected 10 unique trials\n",
    "    if len(unique_trials) == n:\n",
    "        break\n",
    "\n",
    "top_n = [trial.params for trial in unique_trials]\n",
    "\n",
    "config_c1s = [str(i) + \"-\" + config[\"c1\"]for i,config in zip(range(len(top_n)), top_n)]\n",
    "config_c2s = [str(i) + \"-\" + config[\"c2\"]for i,config in zip(range(len(top_n)), top_n)]\n",
    "config_downsample = [10] * n\n",
    "config_operation = [config[\"operation\"] for config in top_n]\n",
    "config_window = [config[\"window_size\"] for config in top_n]\n",
    "config_rolling = [config[\"rolling\"] for config in top_n]\n",
    "config = [config_c1s, config_c2s, config_downsample, config_operation, config_window, config_rolling]\n",
    "train_config_double(X_train, X_test, y_train, y_test, config)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-19T14:01:05.733541Z"
    }
   },
   "id": "d3d32d5b8f01dc49",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run with the best parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2035b61d18f0e47a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T13:57:12.914208Z",
     "start_time": "2024-03-18T13:57:12.912824Z"
    }
   },
   "id": "2d7cb89aeaf9601",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
