{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:27:43.511985Z",
     "start_time": "2024-03-15T11:27:42.588988Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src.modules.training.datasets.main_dataset import MainDataset\n",
    "from src.utils.setup import setup_data\n",
    "# Import all important ML packages.\n",
    "from src.utils.stratified_splitter import StratifiedSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788bad076c577a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## EEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b78e6100b920",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load in all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb9be330181969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:27:55.187348Z",
     "start_time": "2024-03-15T11:27:43.512854Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "raw_path = Path(\"../../data/raw\")\n",
    "metadata_path = raw_path / \"train.csv\"\n",
    "eeg_path = raw_path / \"train_eegs\"\n",
    "spectrogram_path = raw_path / \"train_spectrograms\"\n",
    "cache = Path(\"../../data/cache\")\n",
    "\n",
    "#Start by setting up the data.\n",
    "X, y = setup_data(metadata_path, eeg_path, spectrogram_path, cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57883564ecd3932c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c76418d9f5bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:11.960525Z",
     "start_time": "2024-03-15T11:27:55.188179Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "#Sample 10000 random indices from the training dataset and 2000 from the validation dataset.\n",
    "random.seed(42)\n",
    "\n",
    "#Pickle X_train, X_test, y_train, y_test  as 1 tuple if it does not exist yet.\n",
    "n_samples = 5000\n",
    "\n",
    "eeg_path = \"../../data/raw/train_eegs\"\n",
    "meta_path = \"../../data/raw/train.csv\"\n",
    "cache_path = \"../../data/cache\"\n",
    "\n",
    "if not os.path.exists(\"train_test20k.pkl\") or n_samples != 20000:\n",
    "    #Start by setting up the data. Do not run if you already have the pickled data. \n",
    "\n",
    "    X, y = setup_data(eeg_path=eeg_path, metadata_path=meta_path, cache_path=cache_path, spectrogram_path=None)\n",
    "    \n",
    "    #Create a dataset\n",
    "    splitter = StratifiedSplitter(n_splits=5)\n",
    "    train_indices, test_indices = splitter.split(X.meta, y)[0]\n",
    "    \n",
    "    train_dataset = MainDataset(\"eeg\")\n",
    "    test_dataset = MainDataset(\"eeg\")\n",
    "    \n",
    "    train_dataset.setup(X, y, train_indices, subsample_data=True)\n",
    "    test_dataset.setup(X, y, test_indices, subsample_data=True)\n",
    "    \n",
    "    train_sample = random.sample(list(range(len(train_dataset.indices))), n_samples)\n",
    "    val_sample = random.sample(list(range(len(test_dataset.indices))), len(test_dataset.indices))\n",
    "\n",
    "    X_train = np.array([train_dataset.__getitem__(i)[0] for i in train_sample])\n",
    "    y_train = np.array([train_dataset.__getitem__(i)[1] for i in train_sample])\n",
    "\n",
    "    X_test = np.array([test_dataset.__getitem__(i)[0] for i in val_sample])\n",
    "    y_test = np.array([test_dataset.__getitem__(i)[1] for i in val_sample])\n",
    "    with open(\"train_test20k.pkl\", \"wb\") as f:\n",
    "        pickle.dump((X_train, X_test, y_train, y_test), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(\"train_test20k.pkl\", \"rb\") as f:\n",
    "        X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898ba1587cfdce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bc341fc99903a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:11.963129Z",
     "start_time": "2024-03-15T11:28:11.961648Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try statistical features such as rolling mean to start with on all channels \n",
    "# and then try to use the rolling mean on the channels that are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4843c923ad206",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.017523Z",
     "start_time": "2024-03-15T11:28:11.963634Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def channel_selection(data, channels):\n",
    "    \"\"\"\n",
    "    Selects the specified channels from a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        channels (list): The list of channels to select.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The data with the selected channels.\n",
    "    \"\"\"\n",
    "    return data[:, :, channels]\n",
    "\n",
    "\n",
    "def apply_downsampling(data, factor):\n",
    "    \"\"\"\n",
    "    Downsamples a 3D numpy array by averaging over windows of the specified factor.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        factor (int): The downsampling factor.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The downsampled data.\n",
    "    \"\"\"\n",
    "    reshaped_data = data.reshape(data.shape[0], data.shape[1] // factor, factor, data.shape[2])\n",
    "\n",
    "    # Compute the mean along the new axis that represents the blocks to be averaged\n",
    "    downsampled_data = reshaped_data.mean(axis=2)\n",
    "\n",
    "    return downsampled_data\n",
    "\n",
    "\n",
    "def apply_rolling_operation(data, window_size, operation):\n",
    "    \"\"\"\n",
    "    Applies a rolling operation (e.g., mean, std, min, max) to a 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The input data of shape (samples, sequence_length, channels).\n",
    "        window_size (int): The size of the rolling window.\n",
    "        operation (callable): The operation to apply within the rolling window.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The data with the rolling operation applied.\n",
    "    \"\"\"\n",
    "    # Initialize the result array with NaNs to preserve original shape\n",
    "    result = np.full(data.shape, np.nan)\n",
    "\n",
    "    # Apply the rolling operation to each sample and channel\n",
    "    for sample_idx in tqdm(range(data.shape[0])):\n",
    "        for channel_idx in range(data.shape[2]):\n",
    "            sequence = data[sample_idx, :, channel_idx]\n",
    "            # Use sliding_window_view to create rolling windows\n",
    "            windows = sliding_window_view(sequence, window_shape=window_size)\n",
    "            # Apply the operation across the windows' axis (axis=-1) and assign to the result\n",
    "            # Center the result of the operation in the middle of the window\n",
    "            operation_result = operation(windows, axis=-1)\n",
    "            start_index = window_size // 2\n",
    "            end_index = start_index + operation_result.shape[0]\n",
    "            result[sample_idx, start_index:end_index, channel_idx] = operation_result\n",
    "\n",
    "            #Forward and backward fill the NaNs\n",
    "            result[sample_idx, :, channel_idx] = pd.Series(result[sample_idx, :, channel_idx]).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def rolling_plot(X_train, X_test, window_size, operation):\n",
    "    #Create a plot before and after.\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "    #Plot the original data for the first sample and channel\n",
    "    ax[0].plot(X_train[0, :, 0])\n",
    "    ax[0].set_title(\"Original data\")\n",
    "\n",
    "    # Assuming X_train and X_test are your datasets\n",
    "    X_train = apply_rolling_operation(X_train, window_size, operation)\n",
    "    X_test = apply_rolling_operation(X_test, window_size, operation)\n",
    "\n",
    "    #Plot the transformed data for the first sample and channel\n",
    "    ax[1].plot(X_train[0, :, 0])\n",
    "    ax[1].set_title(f\"Transformed data - rolling {str(operation)} with window size {window_size}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95071060c091b30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65eff39e14c08a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.086941Z",
     "start_time": "2024-03-15T11:28:12.018190Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove the time dimension by flattening the data from (n, sequence_length, channels) to (n*sequence_length, channels).\n",
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    sequence_length = X_train.shape[1]\n",
    "\n",
    "    X_train = X_train.reshape(-1, X_train.shape[2])\n",
    "    X_test = X_test.reshape(-1, X_test.shape[2])\n",
    "\n",
    "    #Also process the labels from size (n, channels) to (n*sequence_length, channels).\n",
    "    y_train = np.repeat(y_train, repeats=sequence_length, axis=0)\n",
    "    y_test = np.repeat(y_test, repeats=sequence_length, axis=0)\n",
    "\n",
    "    #Clip the data between -1024 and 1024.\n",
    "    X_train = np.clip(X_train, -1024, 1024)\n",
    "    X_test = np.clip(X_test, -1024, 1024)\n",
    "\n",
    "    #Divide the data by 32\n",
    "    X_train = X_train / 32\n",
    "    X_test = X_test / 32\n",
    "\n",
    "    #Set NaN to 0\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    X_test = np.nan_to_num(X_test)\n",
    "\n",
    "    #Divide the labels by the sum of the labels to get a probability distribution.\n",
    "    y_train = y_train / np.sum(y_train, axis=1).reshape(-1, 1)\n",
    "    y_test = y_test / np.sum(y_test, axis=1).reshape(-1, 1)\n",
    "\n",
    "    #Randomly shuffle train data\n",
    "    np.random.seed(42)\n",
    "    p = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[p]\n",
    "    y_train = y_train[p]\n",
    "\n",
    "    #One hot encode the labels\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b51909633014b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.138554Z",
     "start_time": "2024-03-15T11:28:12.087519Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def train(X_train, X_test, y_train, y_test):\n",
    "    #Create a model\n",
    "    model = xgb.XGBClassifier(num_class=6, n_estimators=100, max_depth=10, learning_rate=0.1, verbosity=2, early_stopping_rounds=10)\n",
    "    # Use early stopping to stop the training if the validation score does not improve for 10 rounds.\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a51cfa73a4ee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.165322Z",
     "start_time": "2024-03-15T11:28:12.139216Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate the accuracy and create a confusion matrix with seaborn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.scoring.kldiv import KLDiv\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    #Set the axis labels\n",
    "    #Create the confusion matrix\n",
    "    #Take the argmax of the probabilities to get the predicted class\n",
    "    y_test_final = np.argmax(y_test, axis=1)\n",
    "    y_pred_final = np.argmax(y_pred, axis=1)\n",
    "    print(y_test.shape, y_pred.shape)\n",
    "    kldiv = KLDiv()\n",
    "    print(f\"KLDiv: {kldiv(y_test, y_pred)}\")\n",
    "    print(f\"Classification accuracy:  {accuracy_score(y_test_final, y_pred_final)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    sns.heatmap(confusion_matrix(y_test_final, y_pred_final), annot=True, fmt=\"d\", ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    #Set the labels\n",
    "    ax.set_xticklabels([\"Seizure\", \"Lpd\", \"Gpd\", \"Lrda\", \"Grda\", \"Other\"])\n",
    "    ax.set_yticklabels([\"Seizure\", \"Lpd\", \"Gpd\", \"Lrda\", \"Grda\", \"Other\"])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ed8f109f98fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.206099Z",
     "start_time": "2024-03-15T11:28:12.165939Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names, tsfresh=False):\n",
    "    #Plot the feature importances with xgboost horizontally\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(35, 10))\n",
    "\n",
    "    xgb.plot_importance(model, ax=ax[0], importance_type=\"weight\")\n",
    "    if not tsfresh:\n",
    "        ax[0].set_yticklabels(feature_names)\n",
    "\n",
    "    #Show plots/brain.png on the right\n",
    "    ax[1].imshow(plt.imread(\"../../plots/brain.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979af24a1eeba38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.248655Z",
     "start_time": "2024-03-15T11:28:12.207138Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline(X_train, X_test, y_train, y_test):\n",
    "    #Channel selection\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    selected_c = all_channels\n",
    "    channels = [channel_dict[c] for c in selected_c]\n",
    "    window_size = 175\n",
    "    operation = np.std\n",
    "\n",
    "    #Select the channels\n",
    "    X_train = channel_selection(X_train, channels)\n",
    "    X_test = channel_selection(X_test, channels)\n",
    "\n",
    "    #Downsample the data\n",
    "    X_train = apply_downsampling(X_train, 10)\n",
    "    X_test = apply_downsampling(X_test, 10)\n",
    "\n",
    "    #Apply the rolling operation\n",
    "    X_train, X_test = rolling_plot(X_train, X_test, window_size, operation)\n",
    "\n",
    "    #Now preprocess the data\n",
    "    X_train, X_test, y_train, y_test = preprocess(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #Train the model\n",
    "    model = train(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    #Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #Plot the confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #Plot the feature importances\n",
    "    feature_names = np.array(X.eeg[list(X.eeg.keys())[0]].columns)[channels]\n",
    "    plot_feature_importances(model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725af528db6c8d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.290531Z",
     "start_time": "2024-03-15T11:28:12.249249Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the pipeline\n",
    "#pipeline(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a3cd91923b4a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:12.336373Z",
     "start_time": "2024-03-15T11:28:12.291055Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets use tsfresh for feature engineering\n",
    "\n",
    "def preprocess_to_df(X, all_channels):\n",
    "    \n",
    "    #Clip the data between -1024 and 1024.\n",
    "    X = np.clip(X, -1024, 1024)\n",
    "    #Divide the data by 32\n",
    "    X = X / 32\n",
    "    #Set NaN to 0\n",
    "    X = np.nan_to_num(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    id = np.repeat(range(X.shape[0]), X.shape[1])\n",
    "    X = X.reshape(-1, X.shape[2])\n",
    "    df = pd.DataFrame(X, columns=all_channels)\n",
    "    del X\n",
    "    df[\"id\"] = id\n",
    "    return df\n",
    "\n",
    "def apply_tsfresh(df):\n",
    "    from tsfresh import extract_features\n",
    "    from tsfresh.feature_extraction import MinimalFCParameters\n",
    "    \n",
    "    #Extract the features\n",
    "    features = extract_features(df, column_id=\"id\", default_fc_parameters=MinimalFCParameters())\n",
    "    return features\n",
    "\n",
    "def preprocess_tf(y):\n",
    "    #One hot encode the labels\n",
    "    y = np.argmax(y, axis=1)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8928597678316",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:28:13.520875Z",
     "start_time": "2024-03-15T11:28:12.336915Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tsfresh import select_features\n",
    "\n",
    "\n",
    "def pipeline_tf(X_train, X_test, y_train, y_test):\n",
    "    #Channel selection\n",
    "    all_channels = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "\n",
    "    channel_indices = range(len(all_channels))\n",
    "    channel_dict = dict(zip(all_channels, channel_indices))\n",
    "    selected_c = all_channels\n",
    "    channels = [channel_dict[c] for c in selected_c]\n",
    "\n",
    "    #Select the channels\n",
    "    X_train = channel_selection(X_train, channels)\n",
    "    X_test = channel_selection(X_test, channels)\n",
    "\n",
    "    #Downsample the data\n",
    "    X_train = apply_downsampling(X_train, 10)\n",
    "    X_test = apply_downsampling(X_test, 10)\n",
    "\n",
    "    #Apply the rolling operation\n",
    "    X_train = preprocess_to_df(X_train, all_channels)\n",
    "    X_test = preprocess_to_df(X_test, all_channels)\n",
    "    \n",
    "    X_train = apply_tsfresh(X_train)\n",
    "    X_test = apply_tsfresh(X_test)\n",
    "    \n",
    "    print(f\"X_train shape after feature extraction: {X_train.shape}\")\n",
    "    #Now preprocess the data\n",
    "    y_train_final = preprocess_tf(y_train)\n",
    "    y_test_final = preprocess_tf(y_test)\n",
    "    \n",
    "    # Perform feature selection\n",
    "    X_train = select_features(X_train, y_train_final)\n",
    "    #Get the column names of the relevant features\n",
    "    relevant_features = list(X_train.columns)\n",
    "\n",
    "    #Slice X_test to only include the relevant features\n",
    "    X_test = X_test[relevant_features]\n",
    "    print(f\"X_train shape after feature selection: {X_train.shape}\")\n",
    "    \n",
    "    #Train the model\n",
    "    model = train(X_train, X_test, y_train_final, y_test_final)\n",
    "\n",
    "    #Make predictions with probabilities\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "\n",
    "    #Plot the confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #Plot the feature importances\n",
    "    feature_names = np.array(X.eeg[list(X.eeg.keys())[0]].columns)[channels]\n",
    "    plot_feature_importances(model, feature_names, tsfresh=True)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37330529ae928d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T11:32:40.767357Z",
     "start_time": "2024-03-15T11:28:13.521663Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Run the pipeline\n",
    "model, y_pred = pipeline_tf(X_train, X_test, y_train, y_test)                          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
