defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps:
    - _target_: src.modules.transformation.eeg.clip.ClipEEG
      lower: -1024
      upper: 1024
    - _target_: src.modules.transformation.nantozero.NaNToZero
      eeg: true
      kaggle_spec: false
    - _target_: src.modules.transformation.eeg.divide.Divide
      value: 32
    - _target_: src.modules.transformation.eeg.butter.ButterFilter
    - _target_: src.modules.transformation.spectrogram.eeg_to_spectrogram.EEGToSpectrogram
      size: [128, 320]
      fitting_method: 'pad'

y_sys:
  steps:
    - _target_: src.modules.transformation.target.sum_to_one.SumToOne

train_sys:
  steps:
    - _target_: src.modules.training.main_trainer.MainTrainer
      model:
        _target_: src.modules.training.models.timm.Timm
        in_channels: 4
        out_channels: 6
        model_name: "efficientnet_b0"
      optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.Adam
        lr: 5e-5
      criterion:
        _target_: src.modules.training.losses.kldiv.CustomKLDivLoss
        reduction: "batchmean"
        weighted: false
      epochs: 10
      batch_size: 32
      patience: 30
      dataset:
        _target_: src.modules.training.datasets.main_dataset.MainDataset
        data_type: "eeg_spec"
        augmentations: 
          _target_: kornia.augmentation.AugmentationSequential
          _args_:
            - _target_: kornia.augmentation.RandomHorizontalFlip
              p: 0.5
            - _target_: kornia.augmentation.RandomContrast
              p: 0.5
            - _target_: kornia.augmentation.RandomBrightness
              p: 0.5
            - _target_: kornia.augmentation.RandomGaussianBlur
              p: 0.5
              kernel_size: [3, 3]
              sigma: [0.1, 2.0]
      scheduler:
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: timm.scheduler.cosine_lr.CosineLRScheduler
        t_initial: 60
        cycle_mul: 1
        cycle_decay: 1
        cycle_limit: 1
        warmup_t: 20
        warmup_lr_init: 1e-5

