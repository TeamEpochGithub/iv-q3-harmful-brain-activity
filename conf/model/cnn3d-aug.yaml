defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps:
    - _target_: src.modules.transformation.clip.Clip
      lower: -1024
      upper: 1024
      eeg: true
    - _target_: src.modules.transformation.nantozero.NaNToZero
      eeg: true
    - _target_: src.modules.transformation.eeg.divide.Divide
      value: 32
    - _target_: src.modules.transformation.eeg.butter.ButterFilter
      lower: 0.5
      method: "extend"
      ranges: [[1,4],[4,8],[8,12],[12,25]]
    - _target_: src.modules.transformation.eeg.quantize.Quantizer
    - _target_: src.modules.transformation.eeg.downsample.Downsample
      downsample_factor: 10
#    - _target_: src.modules.transformation.eeg.rolling.Rolling
#      channels: [0, 2, 4, 6]
#      window_sizes: [20, 20, 20, 20]
#      operations: ["std", "std", "std", "std"]
#    - _target_: src.modules.transformation.eeg.select_channels.SelectChannels
#      channels: [8, 9, 10, 11, 12, 13, 14, 15] #Select the last 18 channels

train_sys:
  steps:
    - _target_: src.modules.training.main_trainer.MainTrainer
      model_name: EEGNet # Can't have special characters or spaces
      two_stage: true
      two_stage_evaluator_threshold: 9
      two_stage_split_test: true
      two_stage_pretrain_full: true
      model:
        _target_: src.modules.training.models.cnn3d.model.Model
        in_channels: 4
        out_channels: 6
        model_type: "efficientnet-b0"
      optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.AdamW
        lr: 0.003
        weight_decay: 1e-2
      criterion:
        _target_: src.modules.training.losses.kldiv_logits.CustomKLDivLogitsLoss
      epochs: 50
      batch_size: 32
      patience: 15
      dataset_args:
        data_type: "eeg"
        get_item_custom:
        subsample_method: "running_random"
        augmentations:
          _target_: src.modules.training.augmentations.custom_sequential.CustomSequential
          x_transforms:
            - _target_: src.modules.training.augmentations.reverse_1d.Reverse1D
              p: 0.22801295284801437
            - _target_: torch_audiomentations.Shift
              p: 0.21820304460060477
              rollover: true
              mode: per_example
            - _target_: src.modules.training.augmentations.random_phase_shift.RandomPhaseShift
              p: 0.4
              shift_limit: 0.25
            - _target_: src.modules.training.augmentations.substract_channels.SubstractChannels
              p: 0.2
          xy_transforms:
            - _target_: src.modules.training.augmentations.mixup_1d.MixUp1D
              p: 0.22
            - _target_: src.modules.training.augmentations.cutmix_1d.CutMix1D
              p: 0.12
              # Randomly select a percentage between 'low' and 'high' to preserve on the left side of the signal.
              low: 0.07886719060925124
              high: 0.5711317254713427
    - _target_: src.modules.training.postprocessing.softmax.Softmax
    - _target_: src.modules.training.postprocessing.smooth_patient.SmoothPatient
      smooth_factor: 0.1

