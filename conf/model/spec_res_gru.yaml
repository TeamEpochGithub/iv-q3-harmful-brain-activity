defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps:
    - _target_: src.modules.transformation.clip.Clip
      lower: -1024
      upper: 1024
      eeg: true
    - _target_: src.modules.transformation.nantozero.NaNToZero
      eeg: true
    - _target_: src.modules.transformation.eeg.divide.Divide
      value: 32
    - _target_: src.modules.transformation.eeg.butter.ButterFilter

    - _target_: src.modules.transformation.eeg.downsample.Downsample
      downsample_factor: 5

    # Kaggle Spectrogram Transformation
    - _target_: src.modules.transformation.clip.Clip
      kaggle_spec: true
      lower: 0.01
      upper: 3000

    - _target_: src.modules.transformation.spectrogram.log.Log
      kaggle_spec: true

    - _target_: src.modules.transformation.spectrogram.standardize.Standardize
      kaggle_spec: true

    - _target_: src.modules.transformation.nantozero.NaNToZero
      kaggle_spec: true

    - _target_: src.modules.transformation.spectrogram.pad.Pad
      kaggle_spec: true
      pad_list: [0, 0, 14, 14]

    # EEG Transformation
    - _target_: src.modules.transformation.clip.Clip
      eeg: true
      lower: -1024
      upper: 1024

    - _target_: src.modules.transformation.nantozero.NaNToZero
      eeg: true

    # EEG Spectrogram Transformation
    - _target_: src.modules.transformation.spectrogram.eeg_to_spectrogram.EEGToSpectrogram
      size: [100, 256]
      fitting_method: 'crop'

    - _target_: src.modules.transformation.spectrogram.pad.Pad
      eeg_spec: true
      pad_list: [0, 0, 14, 14]

y_sys:
  steps:
    - _target_: src.modules.transformation.target.sum_to_one.SumToOne

train_sys:
  steps:
    - _target_: src.modules.training.main_trainer.MainTrainer
      model_name: q1_spec_res_gru
      model:
        _target_: src.modules.training.models.spectrogram_cnn_gru.MultiResidualBiGRUwSpectrogramCNN
        in_channels: 20
        out_channels: 6
      optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.Adam
        lr: 0.0001
      criterion:
        _target_: src.modules.training.losses.weighted_kldiv.WeightedKLDivLoss
      epochs: 10
      batch_size: 32
      patience: 10
      dataset:
        _target_: src.modules.training.datasets.main_dataset.MainDataset
        data_type: "custom"
        get_item_custom: 
          _target_: src.modules.training.datasets.get_item.chris_and_eeg.ChrisandEEGGetItem
      # scheduler:
      #   _target_: functools.partial
      #   _args_:
      #     - _target_: hydra.utils.get_class
      #       path: timm.scheduler.cosine_lr.CosineLRScheduler
      #   t_initial: 25
      #   cycle_mul: 1
      #   cycle_decay: 1
      #   cycle_limit: 1
      #   warmup_t: 1
      #   warmup_lr_init: 0.000001