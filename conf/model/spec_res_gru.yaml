defaults:
  - pipeline/default@_here_
  - _self_

x_sys:
  steps:
    - _target_: src.modules.transformation.clip.Clip
      lower: -1024
      upper: 1024
      eeg: true
    - _target_: src.modules.transformation.nantozero.NaNToZero
      eeg: true
    - _target_: src.modules.transformation.eeg.divide.Divide
      value: 32
    - _target_: src.modules.transformation.eeg.butter.ButterFilter

    - _target_: src.modules.transformation.eeg.downsample.Downsample
      downsample_factor: 5


y_sys:
  steps:
    - _target_: src.modules.transformation.target.sum_to_one.SumToOne
    - _target_: src.modules.transformation.target.gaussian_target.GaussianTarget
      labels_length: 2000
      sigma: 2 # in seconds

train_sys:
  steps:
    - _target_: src.modules.training.main_trainer.MainTrainer
      model_name: q1_spec_res_gru
      model:
        _target_: src.modules.training.models.spectrogram_cnn_gru.MultiResidualBiGRUwSpectrogramCNN
        in_channels: 20
        out_channels: 6
      optimizer: # Partially instantiate optimizer, so model parameters can be linked at runtime
        _target_: functools.partial
        _args_:
          - _target_: hydra.utils.get_class
            path: torch.optim.Adam
        lr: 0.0001
      criterion:
        _target_: src.modules.training.losses.weighted_kldiv.WeightedKLDivLoss
        # class_weights: [0.767737189589508, 0.8769737036801599, 1.133067562494455, 1.8903799535040144, 1.0400019137008414, 0.29183967703102165]
      epochs: 101
      batch_size: 64
      patience: 90
      dataset:
        _target_: src.modules.training.datasets.main_dataset.MainDataset
        data_type: "eeg"
      # scheduler:
      #   _target_: functools.partial
      #   _args_:
      #     - _target_: hydra.utils.get_class
      #       path: timm.scheduler.cosine_lr.CosineLRScheduler
      #   t_initial: 25
      #   cycle_mul: 1
      #   cycle_decay: 1
      #   cycle_limit: 1
      #   warmup_t: 1
      #   warmup_lr_init: 0.000001