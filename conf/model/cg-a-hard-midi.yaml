_target_: epochalyst.pipeline.model.model.ModelPipeline
_convert_: partial
x_sys:
  _target_: src.modules.transformation.verbose_transformation_pipeline.VerboseTransformationPipeline
  title: Preprocessing pipeline
  steps:
  - _target_: src.modules.transformation.eeg.circumferential.Circumferential
    str_map: CZ_REFERENCE
    keep_ekg: false
  - _target_: src.modules.transformation.clip.Clip
    lower: -1024
    upper: 1024
    eeg: true
  - _target_: src.modules.transformation.nantozero.NaNToZero
    eeg: true
  - _target_: src.modules.transformation.eeg.divide.Divide
    value: 32
  - _target_: src.modules.transformation.eeg.butter.ButterFilter
    lower: 0.5
    method: extend
    ranges:
    - - 1
      - 4
    - - 4
      - 8
    - - 8
      - 12
    - - 12
      - 25
  - _target_: src.modules.transformation.eeg.quantize.Quantizer
  - _target_: src.modules.transformation.eeg.downsample.Downsample
    downsample_factor: 5
y_sys:
  _target_: src.modules.transformation.verbose_transformation_pipeline.VerboseTransformationPipeline
  title: Label processing pipeline
  steps: []
train_sys:
  _target_: src.modules.training.verbose_training_pipeline.VerboseTrainingPipeline
  steps:
  - _target_: src.modules.training.main_trainer.MainTrainer
    model_name: MultiResidualBiGRU
    two_stage: true
    two_stage_evaluator_threshold: 9
    two_stage_split_test: true
    two_stage_pretrain_full: true
    test_split_type: 5
    model:
      _target_: src.modules.training.models.parkinson_multi_res_bi_gru.MultiResidualBiGRU
      input_size: 72
      hidden_size: 64
      out_size: 6
      n_layers: 2
    optimizer:
      _target_: functools.partial
      _args_:
      - _target_: hydra.utils.get_class
        path: torch.optim.AdamW
      lr: 0.00258392391366189
    criterion:
      _target_: src.modules.training.losses.kldiv_logits.CustomKLDivLogitsLoss
      reduction: batchmean
    epochs: 60
    batch_size: 32
    patience: 15
    dataset_args:
      subsample_method: running_random
      data_type: eeg
      augmentations:
        _target_: src.modules.training.augmentations.custom_sequential.CustomSequential
        x_transforms:
        - _target_: src.modules.training.augmentations.reverse_1d.Reverse1D
          p: 0.23
        - _target_: torch_audiomentations.Shift
          p: 0.2
          rollover: true
          mode: per_example
        - _target_: src.modules.training.augmentations.mirror_1d.Mirror1D
          p: 0.5
        - _target_: src.modules.training.augmentations.random_phase_shift.RandomPhaseShift
          p: 0.15
          shift_limit: 0.25
        - _target_: src.modules.training.augmentations.substract_channels.SubstractChannels
          p: 0.2
        xy_transforms:
        - _target_: src.modules.training.augmentations.mixup_1d.MixUp1D
          p: 0.22
        - _target_: src.modules.training.augmentations.cutmix_1d.CutMix1D
          p: 0.35
          low: 0.1
          high: 0.8
    scheduler:
      _target_: functools.partial
      _args_:
      - _target_: hydra.utils.get_class
        path: timm.scheduler.cosine_lr.CosineLRScheduler
      t_initial: 34
      cycle_mul: 1
      cycle_decay: 1
      cycle_limit: 1
      warmup_t: 7
      warmup_lr_init: 3.899279729965608e-05
  - _target_: src.modules.training.postprocessing.softmax.Softmax
  - _target_: src.modules.training.postprocessing.smooth_patient.SmoothPatient
    smooth_factor: 0.1
pred_sys:
  _target_: src.modules.transformation.verbose_transformation_pipeline.VerboseTransformationPipeline
  title: Postprocessing pipeline
label_sys:
  _target_: src.modules.transformation.verbose_transformation_pipeline.VerboseTransformationPipeline
